
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Neural Networks, Language Embedders, Autocoding– Oh, my! &#8212; Kaggle Feedback Prize Reflections</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Conclusion" href="Conclusion.html" />
    <link rel="prev" title="Ensembles" href="Ensembling.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="_static/MCIXLogo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Kaggle Feedback Prize Reflections</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="Landing.html">
                    Title: Kaggle: Feedback Prize - Predicting Effective Arguments: An Instructive Post-Mortem*
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Body.html">
   Visualizing, Cleaning, Feature Engineering, Mining
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Ensembling.html">
   Exploring Ensembles
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Neural Networks, Language Embedders, Autocoding– Oh, my!
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Conclusion.html">
   Conclusion
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Appendix%201.html">
   Appendix 1, Philosophical Reflections on NLP
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Appendix%202.html">
   Appendix 2, Concerns Regarding the Data
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FNNs.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/NNs.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Neural Networks, Language Embedders, Autocoding– Oh, my!
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-framework">
   Model Framework
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda">
   CUDA
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters-and-user-specifications">
   Hyperparameters and User Specifications
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-details">
   Model Details
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#architecture">
   Architecture
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Neural Networks, Language Embedders, Autocoding– Oh, my!</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Neural Networks, Language Embedders, Autocoding– Oh, my!
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-framework">
   Model Framework
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#cuda">
   CUDA
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#hyperparameters-and-user-specifications">
   Hyperparameters and User Specifications
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-details">
   Model Details
  </a>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#architecture">
   Architecture
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section id="neural-networks-language-embedders-autocoding-oh-my">
<h1>Neural Networks, Language Embedders, Autocoding– Oh, my!<a class="headerlink" href="#neural-networks-language-embedders-autocoding-oh-my" title="Permalink to this headline">#</a></h1>
<p>In the aforementioned-then-subsequently-ignored interest of brevity, I’ve tried to strip these example builds down to only what’s necessary to grasp the implementation strategy; all of the reading-in of files, mapping of labels, importing of modules, and doing of the essential-but-obvious-stuff is excised. I’ve linked, however, to related models, notebooks, articles, and so on, so building and customizing – if that’s of interest– shouldn’t be too hard. Likewise, the outputs, too, are just noted briefly or shown at the bottom, since each of these takes &gt;45 minutes to process on a Kaggle GPU.</p>
<p>Preliminary Concepts I won’t get too in the weeds labeling and describing these things– in no small part because there are many parts that I am myself quite hazy about, at least in terms of ‘why-this-not-that’ and ‘what-difference-does-it-make.’ This isn’t a textbook on neural networks, and I wouldn’t try to author one. If you’re completely unfamiliar with deep learning/neural networks, there is an exhaustive universe of free resources, which I’ll leave to you, dear reader, to explore. A paid resource that I found very helpful is here: <a class="reference external" href="https://www.amazon.com/Deep-Learning-Approach-Andrew-Glassner/dp/1718500726/ref=pd_bxgy_sccl_1/131-4168850-7449845?pd_rd_w=LWxP9&amp;content-id=amzn1.sym.7757a8b5-874e-4a67-9d85-54ed32f01737&amp;pf_rd_p=7757a8b5-874e-4a67-9d85-54ed32f01737&amp;pf_rd_r=SDWF9ERCTD48CSTV0RR8&amp;pd_rd_wg=iNJtJ&amp;pd_rd_r=53d0eb02-2c69-4e0d-b4ae-08f002af1f0d&amp;pd_rd_i=1718500726&amp;psc=1">Link</a>. Briefly, I’ll touch on some of the code-elements that might be unfamiliar. All of these language-processing neural nets have (in model-appropriate variations) the following:</p>
<p>If you’re completely unfamiliar with neural networks, very little of what follows will make sense. But for context, a neural network is a configuration of nodes grouped into layers. These nodes contribute weights to parts of the input data, which are cumulatively fed to nodes in subsequent layers. The subsequent layer’s nodes activate if and only if the input from the previous layer passes a certain threshold. This ultimately arrives at an output layer made of – at least for classification tasks – as many nodes as there are classes in the dependent variable. The activation at this level is the model’s prediction. Other layers have other functions, the input layer, for example, just recieves the data; a ‘dense’ layer is a replicate layer that in which each node recieves input from all nodes in the previous layer; and so on.</p>
</section>
<section id="model-framework">
<h1>Model Framework<a class="headerlink" href="#model-framework" title="Permalink to this headline">#</a></h1>
<p>In order for neural networks to work, they have to execute <em>A Lot</em> of calculations as the number of weights to be learned increase exponentially with each additional layer in the model. Note that when I say ‘a lot,’ I mean a lot even for computers. Thus configuring the data types and optimizing their implementation on various hardward components requires a good bit of computer engineering know-how. Thankfully, there are open-source libraries that facilitate all of this for us. The two biggest players in this space are indisputably Tensorflow and Torch. Tensorflow is usually used through Keras’ API, while Pytorch is a Python-friendly Torch spinoff that kind of does it all. Since both TF and Pytorch utilize tensors and/or .tsv files, there are methods employed in the following demos that may look a bit unfamiliar; largely these are processes converting the dataset into TF/Torch-amenable formats. Likewise, since these require sequences of a common length, all of the models include some varations of the following:</p>
<ul class="simple">
<li><p>Max length: the maximum size of any given input text</p></li>
<li><p>Truncation: rules specifying to cut off the ends of any overly long texts.</p></li>
<li><p>Padding: rules specifying ‘pad’ tokens to append to texts that aren’t <em>sufficiently</em> long.</p></li>
<li><p>Data Loader: wraps an iterable around the prepped dataset to facilitate access to the contents. It is responsible for selection of items that go into a given batch.</p></li>
<li><p>Dataset: a torch class used to define custom datasets</p></li>
<li><p>Dict(): We frequently have to ‘return_dict’ or ‘dict(list())’ and so on because of the way organize our dataframes into tensors.</p></li>
<li><p>.tsv: tab-separated values. As with dict() and dataloader and all, this is invoked to create the tensors needed for these models and the GPUs they run on.</p></li>
<li><p>Return Tensors = ‘pt’: make Pytorch-amenable tensors.</p></li>
<li><p>‘tf’ + anything: TensorFlow</p></li>
<li><p>Def forward(): A function defining how the model should proceed.</p></li>
</ul>
<p>You’ll also see:</p>
<ul class="simple">
<li><p>model.train()/.eval() – setting the model to training/evaluation mode to inform whether or not it should update. When in eval, the inputs are read-only.</p></li>
<li><p>with __ no_grad() – set during evaluation to instruct the model not to update the gradients</p></li>
<li><p>metrics– library of evaluation methods for use with neural networks.</p></li>
<li><p>nn – to instantiate a neural network from a base class.</p></li>
<li><p>tensor – to build a tensor data matrix.</p></li>
<li><p>Autotune – an algorithm that optimizes processor allocation.</p></li>
<li><p>Verbose – specifies how much of the training progress/information to make available to the analyst.</p></li>
</ul>
</section>
<section id="cuda">
<h1>CUDA<a class="headerlink" href="#cuda" title="Permalink to this headline">#</a></h1>
<p>Everything related to CUDA deals with configurations vis-a-vis GPU processing. If run on Kaggle, you can use their GPUs, otherwise you can a) buy one, b) use one you didn’t know you had (!), c) use another host like Google Colab or Paperspace Gradient, or d) go harangue some wealthier fellow data scientist and convince them to let you ‘borrow’ theirs. Should you decide to try and implement any configuration on your own device, be forewarned: it is <em>not</em> (in the author’s humble opinion) a straightforward, intuitive project…</p>
<ul class="simple">
<li><p>Set Seed – used to make the model reproducible despite using randomly generated values in training</p></li>
<li><p>to_device – defining a variable ‘device’ as either the GPU or CPU, to_device ensures that model and data are sent to the same place.</p></li>
<li><p>n_jobs – specifies how many core processors to allocate to the process. There are plenty of great resources out there for learning deeply about deep learning. On the Tensorflow-Keras-(Py)Torch topic, here’s a good resource (Tip o’ the Hat to Dr. Rogel-Salazar): <a class="reference external" href="https://www.dominodatalab.com/blog/tensorflow-pytorch-or-keras-for-deep-learning">Link.</a></p></li>
</ul>
</section>
<section id="hyperparameters-and-user-specifications">
<h1>Hyperparameters and User Specifications<a class="headerlink" href="#hyperparameters-and-user-specifications" title="Permalink to this headline">#</a></h1>
<p>Model implementation parameters: whatever kind of model we pass, the following helper functions and hyperparameters must be specified:</p>
<ul class="simple">
<li><p>Number-of-Epochs – how many times we fully pass through the dataset in training</p></li>
<li><p>Batch Size – how many text-items to process at a time; more will speed-up training, but can easily exhast memory.</p></li>
<li><p>Iteration – a full forward-and-backward pass through a batch, equal to N/batch-size. Each time data passes forwards, it is analyzed and predicted; based on its performance, the backward-pass (aka ‘backpropagation’) is where the weights are adjusted based on the model’s performance. If you’ve heard of the gradient descent algorithm, the backward pass is when it’s implemented, essentially sending feedback through the network, and moving the weights in such a way as to reduce the loss contributed to the model. (Sometimes called a ‘gradient update.’)</p></li>
<li><p>Loss Function – basically what metric we are using to ‘score’ how well the model is doing. At different weights, the loss (error– the output of the loss function given that particular weight) of the model goes up or down. We want the loss to go down as much as possible (‘descent’).</p></li>
<li><p>Objective/Objective Function – this is the function that the model is attempting to optimize. If a loss-function is defined, the loss function is the objective; if, on the other hand, the objective is accuracy, then we will attempt to <em>maximize</em> it.</p></li>
<li><p>Learning Rate – In seeking to minimize loss, the model ‘descends’ down the gradient of the loss function, by taking ‘steps.’ The size of the steps it takes is the model’s learning rate. If the learning rate is very high, the model takes giant leaps in reconfiguration, and doesn’t steadily descend. If the learning rate is very low, the model is so cautious that it doesn’t make adequate progress.</p>
<ul>
<li><p>Momentum: Specification of the <em>direction</em> the optimizer should head.</p></li>
<li><p>Warmup Steps: Specification of some number of steps in which the learning/updating is inactive. Sort of a ‘let-the-model-get-its-bearings’ period preceding the gradient updating.</p></li>
<li><p>Decay/Weight Decay: for modifying the learning rate – the ‘step-size’ – as we get further along in the analysis. Initially, we may want to take rather large steps, since we began haphazardly. As the model improves, however, we want to slow down and take more cautious steps, since we’ve likely made progress towards the minima of the loss function.</p></li>
<li><p>Learning Rate Scheduler: defines trigger to implement learning rate decay.</p></li>
</ul>
</li>
<li><p>Regularization: A range of tools used to limit the model’s capacity to overfit. Generally, these take the form of some sort of penalty for added complexity. We want the model to balance <em>bias</em> and <em>variance,</em> meaning we want the model to have sufficient complexity to make usable predictions, but not <em>such</em> model-specific detail that it treats anything not in the training set as a non-positive observation. Popular tools in this respect are:</p>
<ul>
<li><p>L1 (aka Lasso): Imposition of a penalty on less-informative features, which can include sending the features all the way to zero (i.e., eliminating them from the model).</p></li>
<li><p>L2 (aka Ridge): Like L1, but never sending any feature’s coefficient to zero. This makes the model more accurate, while still giving a conditional/contextual sense of the importance of individual features.</p></li>
<li><p>Elastic Net: a combination of L1 and L2.</p></li>
<li><p>Early Stopping: Saving the weight configurations from each separate epoch, such that if continued adjustment produces diminishing returns, we can simply deploy an earlier version of the network.</p></li>
<li><p>Dropout: Randomly excluding some observations from exposure to the model in training. This forces the model to assign weights with fewer input features in mind, making the model more generalizable.</p></li>
<li><p>Batch Normalization: transforming the outputs of a given batch such that they are normally distributed.</p></li>
<li><p>Label-Smoothing: A noise-introducing technique that deliberately misshapes the inputs to the final layer so that the model doesn’t become ‘overconfident’ in its predictions at an early stage.</p></li>
</ul>
</li>
<li><p>Activation Function – a function that defines the threshold a previous layer’s inputs must meet in order for a (subsequent layer’s) node to activate. The Rectified Linear Unit (‘ReLU’) is very common– it returns 0 if the input is &lt;=0, and 1 otherwise, where 0 = ‘don’t activate’ and 1 = ‘activate.’</p></li>
<li><p>Optimizer / Optimization Function – defines the specific implementation of the gradient descent algorithm to use. ‘Adam’ – Adaptive Momentum – is very common.</p></li>
<li><p>Criterion – the function used to define boundary thresholds a prediction must meet in order for an item to be predicted a member of class X.</p></li>
<li><p>Entropy – refers to the extent of heterogeneity within any relevant subset of the data. Since informative features are those that <em>distinguish</em> class A from class B, we want less entropy (i.e., we don’t want to evaluate on the basis of features members of class A and class B have in common). If I’m describing a suspect to a police sketch artist, telling him that the person was between 3 and 6 feet tall, had two arms, and was covered in skin isn’t useful, since literally every human meets fits that description. There would be, in that case, a lot of entropy between the classes of ‘The perpetrator’ and ‘Not the perpetrator.’</p>
<ul>
<li><p>Categorical Cross-Entropy – a criterion function that’s based on the degree of entropy between the different classes, in non-binary classification tasks. (Class <em>C</em> ought to be as dissimilar as possible from classes A <em>and</em> B.)</p></li>
</ul>
</li>
<li><p>Shuffle – to remix the batch items after each epoch. Since a random drawing of items may not be representative of the dataset as a whole, re-feeding the same sets over and over could lead to overfitting.</p></li>
<li><p>Step/Step-Size – equivalent to ‘iteration’ and ‘learning rate,’ respectively.</p></li>
<li><p>Argmax: For a function, f, and a set of inputs, X, the argmax is the member of set X, x_i, that produces the greatest output (‘y’-value) of all the items in set X.</p></li>
<li><p>Softmax: an activation function that is essentially the extension of the sigmoid to multi-class, multi-dimensional problems.</p></li>
<li><p>Logits – the log-probabilities output by the model; the softmax function takes these as inputs and transforms them such that they sum to one.</p></li>
<li><p>Normalization: sending all of the values to a predetermined range (say, -1 to 1) via an information-preserving transformation.</p></li>
<li><p>Standardization: sending all of the values to a standard normal distribution.</p></li>
</ul>
</section>
<section id="model-details">
<h1>Model Details<a class="headerlink" href="#model-details" title="Permalink to this headline">#</a></h1>
<p>The specific models themselves, listed below, could each be the subject of entire books, and I won’t attempt to explain them in any detail here. Briefly:</p>
<ul class="simple">
<li><p>Recurrent Neural Network (RNN): a NN that retains a node’s ‘state’ in memory for later use, then ‘forgets’ it at a subsequent step. This type of network is used for sequence-processing, as item N contributes to the meaningfulness of item N+1, and so on. More below. -LSTM: Long Short-Term Memory. A specific sub-type RNN.</p></li>
<li><p>Convolutional Neural Network (CNN): It’s a bit of a challenge to concisely explain anything useful about CNNs. One analogy I found helpful is to think of putting a page from a book under a microscope. The narrowness of the lens requires iteratively sliding the page backwards and forwards, up and down, to read the whole text. In a convolutional NN, the nodes on the convolutional layers scan the input by sliding it back and forth like the book’s pages. These nodes are pattern detecting filters: just as what we see through a microscope is a ‘filtered’ image (in that it is magnified considerably), the nodes are seeing through <em>their</em> filters. These filters overlay a patterned outline, and activate if the input matches the pattern they are looking for. In this way, composition, simplification, and complexity can all be improved.</p>
<ul>
<li><p>Stride: the size of the ‘slides’ that the nodes make (in terms of square units).</p>
<ul>
<li><p>Pooling/Pooled Inputs, Outputs: When a convolutional layer matches a pattern, it can indicate a ‘details-invariant’ match – an indication that it has matched the pattern, even if the pattern is idiosyncratically distinct from other matches. In this way, it can ‘downsample’ the features, essentially simplifying its result to Yes or No. Analogically, when you search for something on Amazon, say, then apply a filter to limit results to a certain brand, there will still be a diversity of items returned, but all will ‘match’ with the brand. A pooling layer summarizes features into matches and non-matches.</p></li>
</ul>
</li>
</ul>
</li>
</ul>
<p>(Tip o’ the Hat to: <a class="reference external" href="https://www.youtube.com/watch?v=YRhxdVk_sIs">Link1.</a> <a class="reference external" href="https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/">Link2.</a>)</p>
</section>
<section id="architecture">
<h1>Architecture<a class="headerlink" href="#architecture" title="Permalink to this headline">#</a></h1>
<p>Since building and training a deep learning network from scratch is a herculean task, almost everyone uses a high-ish level architecture and/or a pretrained model customized to their needs. For NLP tasks, Recurrent Neural Networks or Transformers are most commonly used, as these network-designs permit effective analysis of <em>sequences</em>, which is very important in processing language (after all, ‘the cat sat on the mat’ is a very different sentence from ‘the mat sat on the cat.’). The following items deal with parameters, specifications, and so on apply specifically to formatting and compatibility with the model. Hugging Face is a library from which cutting-edge transformers can be imported, downloaded, customized, etc. ‘Bert’ (Bidirectional Encoder Representations from Transformers) is a SOA model developed by Google that you can access there, and ‘DeBerta’ and ‘RoBerta’ are a couple of their models, though there are many more variations on the core ‘Bert’ architecture. Distilbert is a stripped-down version that allows for speedier training.</p>
<ul class="simple">
<li><p>General Features:</p></li>
<li><p>Tokenization: As above, this is just the division of the sentences into discrete words. There are, however, different approaches to this that are optimized in a model-specific way, so we often specify a model-specific tokenizer.</p></li>
<li><p>Encoding: One of the key features and strengths of these models is their use of encode-decode layers. In this context, encoding means something more like ‘compressing’ than merely converting types. The model attempts to condense the input to the minimum requisite information required to capture what’s being said in the example, and the decode layer does the opposite. Through this process, the model can learn to capture the ‘essence’ of the input text in the smallest possible dimensionality. This allows the model to build the optimal representations, reducing redundancy and noise.</p></li>
<li><p>Embedding: Like vectorization, but more similar to the doc2vec approach. The space into which the vectors are placed is arranged such that similar items are ‘near’ to one another, as discerned by the aformentioned tokenization/encoding process.</p></li>
<li><p>Attention, Attention Heads: Because 1) Language is presented in a sequence: ‘home is where the heart is’ != ‘the heart is where home is.’
\2) Order and context not only matter, they change the meaning of words: ‘after pushing the canoe into the water, he turned and approached the bank’ suggests a very different sort of bank than that in ‘after finsihing the canoe trip, he remembered he had to stop by the bank.’ 3) Ambiguity arises that can only be resolved by order: “I like muffins more than you” could mean, “I like muffins better than I like you” or “I like muffins more than you like muffins”, and only what it’s preceded or followed by can clarify that.</p></li>
<li><p>Bidirectional: Permitting attention and sequential analyis for elements that both precede <em>and</em> follow a given input. This is what attention mechanisms accomplish. From the training corpus, the model learns probabilities of two words appearing in a sequence, probabilities of the word in position + 1 equalling X given that position - 2 = P and position - 1 = Q, and so on. Multi-head attention means simply that the model has a variety of different such features that it weighs when estimating the likelihood of meaning/a subsequent word/a masked word/etc. For example, in addition to positional attention mechanisms, it may have mechanisms that look at parts-of-speech, mechanisms that look for noun-phrases, and so forth.</p></li>
<li><p>Model Specific Features:</p></li>
<li><p>base/large: two different scales of Bert– base has 12 layers, 768 hidden-nodes, 12 attention-heads, 110 million parameters; large uses 24 layers, 1024 hidden nodes, 16 attention heads, and 340 million parameters. Both of these took literally <em>days</em> to train, using multiple TPUs and processing the entire text of Wikipedia!</p></li>
<li><p>Special Tokens: Bert expects certain ‘special’ tokens to be added to the text it is analyzing. These are used as signals to indicate the beginning and end of a given text item (‘CLS’ and ‘SEP’, respectively (with ‘CLS’ being specific to classification tasks)), token-position in terms of the sentence (‘POS’), and training-specific ‘hints’ that help with discerning where to pay attention (‘MASK’ and ‘NSP’).</p>
<ul>
<li><p>Input_ids and Input_masks: variables specifying whether a given token-position consists of an actual token or if it is actually padding that has no informative meaning.</p></li>
</ul>
</li>
<li><p>cased/uncased: just what it sounds like– whether to use a model optimized for text that has conventional uppercase and lowercase usage, or to use a model optimized for data that’s been standardized to lowercase.</p></li>
<li><p>Autotokenizer: a tokenizer (Python) class that builds a model-specific tokenizer based on the model of Bert (or other transformer) you’re using.</p></li>
<li><p>Report_to: integrated platforms on which to save the model</p></li>
<li><p>Save_strategy: whether to save a model after a given iteration, epoch, or whatever.</p></li>
<li><p>Evaluation_strategy: schedule for performing updating An excellent introduction to Bert and related architectures here (Tip o’ the Hat to Samia Khalid): <a class="reference external" href="https://medium.com/&#64;samia.khalid/bert-explained-a-complete-guide-with-theory-and-tutorial-3ac9ebc8fa7c">Link.</a></p></li>
</ul>
<p>Caveat Lector: Exceptions and qualifications exist for just about everything printed in this cell, and the recommended/demonstrated tools are certainly not the only options. There are tons and tons of resources and models and frameworks and configs out there. Those that I’ve used here vary in terms of ease, efficiency, and architecture, but they are all widely used and respected, and multitudinous tutorials, documentation entries, and examples can be found online.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1">#Dependencies List - All Models</span>
<span class="c1">#General Purpose</span>
<span class="kn">import</span> <span class="nn">os</span>
<span class="kn">import</span> <span class="nn">csv</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span>
<span class="kn">import</span> <span class="nn">warnings</span>
<span class="kn">import</span> <span class="nn">datasets</span>
<span class="kn">from</span> <span class="nn">datasets</span> <span class="kn">import</span> <span class="n">load_dataset</span><span class="p">,</span> <span class="n">Dataset</span><span class="p">,</span> <span class="n">DatasetDict</span>
<span class="c1">#Tensorflow</span>
<span class="kn">import</span> <span class="nn">tensorflow</span> <span class="k">as</span> <span class="nn">tf</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Input</span><span class="p">,</span> <span class="n">Dropout</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.optimizers</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">from</span> <span class="nn">tensorflow.keras.models</span> <span class="kn">import</span> <span class="n">Model</span>
<span class="c1">#Transformers</span>
<span class="kn">import</span> <span class="nn">transformers</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">DistilBertTokenizer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFBertModel</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TFDistilBertForSequenceClassification</span> 
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">get_linear_schedule_with_warmup</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AdamW</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertTokenizer</span><span class="p">,</span> <span class="n">BertModel</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">BertForSequenceClassification</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">TrainingArguments</span><span class="p">,</span> <span class="n">Trainer</span>
<span class="kn">from</span> <span class="nn">transformers</span> <span class="kn">import</span> <span class="n">AutoModelForSequenceClassification</span><span class="p">,</span> <span class="n">AutoTokenizer</span>
<span class="c1">#Torch</span>
<span class="kn">import</span> <span class="nn">torch</span> 
<span class="kn">from</span> <span class="nn">torch.utils.data</span> <span class="kn">import</span> <span class="n">DataLoader</span><span class="p">,</span> <span class="n">RandomSampler</span><span class="p">,</span> <span class="n">SequentialSampler</span><span class="p">,</span> <span class="n">TensorDataset</span>
<span class="kn">from</span> <span class="nn">torch</span> <span class="kn">import</span> <span class="n">nn</span>
<span class="kn">from</span> <span class="nn">torch.optim</span> <span class="kn">import</span> <span class="n">Adam</span>
<span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
<span class="c1">#Keras</span>
<span class="kn">from</span> <span class="nn">keras.preprocessing.text</span> <span class="kn">import</span> <span class="n">Tokenizer</span>
<span class="kn">from</span> <span class="nn">keras_preprocessing.sequence</span> <span class="kn">import</span> <span class="n">pad_sequences</span>
<span class="kn">from</span> <span class="nn">keras.models</span> <span class="kn">import</span> <span class="n">Sequential</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dense</span><span class="p">,</span> <span class="n">Embedding</span><span class="p">,</span> <span class="n">LSTM</span><span class="p">,</span> <span class="n">SpatialDropout1D</span>
<span class="kn">from</span> <span class="nn">keras.utils.np_utils</span> <span class="kn">import</span> <span class="n">to_categorical</span>
<span class="kn">from</span> <span class="nn">keras.callbacks</span> <span class="kn">import</span> <span class="n">EarlyStopping</span>
<span class="kn">from</span> <span class="nn">keras.layers</span> <span class="kn">import</span> <span class="n">Dropout</span>
<span class="c1">#Sklearn</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">log_loss</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">StratifiedGroupKFold</span>
</pre></div>
</div>
<p>Bert + Torch v.1</p>
<p>Tip o’ the Hat:</p>
<p><a class="reference external" href="https://medium.com/analytics-vidhya/bert-multi-class-text-classification-e3822836ee0d">Link1.</a> <a class="reference external" href="https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894">Link2.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>

<span class="k">class</span> <span class="nc">Dataset</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">df</span><span class="p">):</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="n">labels</span><span class="p">[</span><span class="n">label</span><span class="p">]</span> <span class="k">for</span> <span class="n">label</span> <span class="ow">in</span> <span class="n">BertPrep</span><span class="p">[</span><span class="s1">&#39;Label&#39;</span><span class="p">]]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">texts</span> <span class="o">=</span> <span class="p">[</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> 
                               <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span> <span class="n">max_length</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                <span class="n">return_tensors</span><span class="o">=</span><span class="s2">&quot;pt&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">BertPrep</span><span class="p">[</span><span class="s1">&#39;Text&#39;</span><span class="p">]]</span>

    <span class="k">def</span> <span class="nf">classes</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span>

    <span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_batch_labels</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">])</span>

    <span class="k">def</span> <span class="nf">get_batch_texts</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">texts</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

    <span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>

        <span class="n">batch_texts</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch_texts</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>
        <span class="n">batch_y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_batch_labels</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">batch_texts</span><span class="p">,</span> <span class="n">batch_y</span>
    
<span class="k">class</span> <span class="nc">BertClassifier</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>

    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.5</span><span class="p">):</span>

        <span class="nb">super</span><span class="p">(</span><span class="n">BertClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">bert</span> <span class="o">=</span> <span class="n">BertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-cased&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">linear</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">768</span><span class="p">,</span> <span class="mi">5</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">relu</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_id</span><span class="p">,</span> <span class="n">mask</span><span class="p">):</span>

        <span class="n">_</span><span class="p">,</span> <span class="n">pooled_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">bert</span><span class="p">(</span><span class="n">input_ids</span><span class="o">=</span> <span class="n">input_id</span><span class="p">,</span> 
                                     <span class="n">attention_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">,</span>
                                     <span class="n">return_dict</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
        <span class="n">dropout_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">pooled_output</span><span class="p">)</span>
        <span class="n">linear_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">linear</span><span class="p">(</span><span class="n">dropout_output</span><span class="p">)</span>
        <span class="n">final_layer</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">linear_output</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">final_layer</span>

<span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">learning_rate</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>

    <span class="n">train</span><span class="p">,</span> <span class="n">val</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">train_data</span><span class="p">),</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">val_data</span><span class="p">)</span>

    <span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> 
                                                   <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> 
                                                   <span class="n">shuffle</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">val_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">val</span><span class="p">,</span> 
                                                 <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
    <span class="n">optimizer</span> <span class="o">=</span> <span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> 
                     <span class="n">lr</span><span class="o">=</span> <span class="n">learning_rate</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>
            <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            <span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>
            
    <span class="k">for</span> <span class="n">epoch_num</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">epochs</span><span class="p">):</span>

            <span class="n">total_acc_train</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total_loss_train</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">for</span> <span class="n">train_input</span><span class="p">,</span> <span class="n">train_label</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>

                <span class="n">train_label</span> <span class="o">=</span> <span class="n">train_label</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">train_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                <span class="n">input_id</span> <span class="o">=</span> <span class="n">train_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_id</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
                
                <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">train_label</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
                <span class="n">total_loss_train</span> <span class="o">+=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                
                <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">train_label</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                <span class="n">total_acc_train</span> <span class="o">+=</span> <span class="n">acc</span>

                <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
                <span class="n">batch_loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
                <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
            
            <span class="n">total_acc_val</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="n">total_loss_val</span> <span class="o">=</span> <span class="mi">0</span>

            <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

                <span class="k">for</span> <span class="n">val_input</span><span class="p">,</span> <span class="n">val_label</span> <span class="ow">in</span> <span class="n">val_dataloader</span><span class="p">:</span>

                    <span class="n">val_label</span> <span class="o">=</span> <span class="n">val_label</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">mask</span> <span class="o">=</span> <span class="n">val_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
                    <span class="n">input_id</span> <span class="o">=</span> <span class="n">val_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

                    <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_id</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

                    <span class="n">batch_loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">val_label</span><span class="o">.</span><span class="n">long</span><span class="p">())</span>
                    <span class="n">total_loss_val</span> <span class="o">+=</span> <span class="n">batch_loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    
                    <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">val_label</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
                    <span class="n">total_acc_val</span> <span class="o">+=</span> <span class="n">acc</span>
                
<span class="k">def</span> <span class="nf">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">test_data</span><span class="p">):</span>

    <span class="n">test</span> <span class="o">=</span> <span class="n">Dataset</span><span class="p">(</span><span class="n">test_data</span><span class="p">)</span>

    <span class="n">test_dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">test</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">use_cuda</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">()</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s2">&quot;cuda&quot;</span> <span class="k">if</span> <span class="n">use_cuda</span> <span class="k">else</span> <span class="s2">&quot;cpu&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_cuda</span><span class="p">:</span>

        <span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">cuda</span><span class="p">()</span>

    <span class="n">total_acc_test</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>

        <span class="k">for</span> <span class="n">test_input</span><span class="p">,</span> <span class="n">test_label</span> <span class="ow">in</span> <span class="n">test_dataloader</span><span class="p">:</span>

            <span class="n">test_label</span> <span class="o">=</span> <span class="n">test_label</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">test_input</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
            <span class="n">input_id</span> <span class="o">=</span> <span class="n">test_input</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

            <span class="n">output</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">input_id</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>

            <span class="n">acc</span> <span class="o">=</span> <span class="p">(</span><span class="n">output</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="o">==</span> <span class="n">test_label</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
            <span class="n">total_acc_test</span> <span class="o">+=</span> <span class="n">acc</span>
        
    
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">112</span><span class="p">)</span>
<span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">df_test</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">BertPrep</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">frac</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">),</span> 
                                     <span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="mf">.8</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">BertPrep</span><span class="p">)),</span> <span class="nb">int</span><span class="p">(</span><span class="mf">.9</span><span class="o">*</span><span class="nb">len</span><span class="p">(</span><span class="n">BertPrep</span><span class="p">))])</span>


<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">BertClassifier</span><span class="p">()</span>
<span class="n">LR</span> <span class="o">=</span> <span class="mf">1e-6</span>
              
<span class="n">train</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df_train</span><span class="p">,</span> <span class="n">df_val</span><span class="p">,</span> <span class="n">LR</span><span class="p">,</span> <span class="n">EPOCHS</span><span class="p">)</span>


<span class="n">evaluate</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">df_test</span><span class="p">)</span>
</pre></div>
</div>
<p>BERT + Torch v.2</p>
<p>Tip o’ the Hat:</p>
<p><a class="reference external" href="https://mccormickml.com/2021/06/29/combining-categorical-numerical-features-with-bert/">Link.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">is_available</span><span class="p">():</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cuda&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">do_lower_case</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
 
<span class="n">model</span> <span class="o">=</span> <span class="n">BertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="n">desc</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">device</span>

<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>          
<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">1e-5</span>           
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">4</span>            
<span class="n">max_len</span> <span class="o">=</span> <span class="mi">0</span>        

<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">feature_sentences</span><span class="o">.</span><span class="n">Token</span><span class="p">:</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
    <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">input_ids</span><span class="p">))</span>

<span class="n">max_len</span> <span class="o">=</span> <span class="mi">250</span> 

<span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">sent</span> <span class="ow">in</span> <span class="n">feature_sentences</span><span class="o">.</span><span class="n">Token</span><span class="p">:</span>
    <span class="n">encoded_dict</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span><span class="n">sent</span><span class="p">,</span> 
                                        <span class="n">add_special_tokens</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                        <span class="n">max_length</span> <span class="o">=</span> <span class="n">max_len</span><span class="p">,</span>
                                         <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                         <span class="n">padding</span> <span class="o">=</span> <span class="s1">&#39;max_length&#39;</span><span class="p">,</span>
                                         <span class="n">return_attention_mask</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                                         <span class="n">return_tensors</span> <span class="o">=</span> <span class="s1">&#39;pt&#39;</span><span class="p">,</span>
                                        <span class="p">)</span>
    <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])</span>
    
    <span class="n">attention_masks</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">encoded_dict</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">])</span>

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">attention_masks</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">attention_masks</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="mi">0</span><span class="p">)</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Labels&#39;</span><span class="p">])</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">attention_masks</span><span class="p">[</span><span class="n">train_idx</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">train_idx</span><span class="p">])</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">val_idx</span><span class="p">],</span> <span class="n">attention_masks</span><span class="p">[</span><span class="n">val_idx</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">val_idx</span><span class="p">])</span>
<span class="n">test_dataset</span><span class="o">=</span> <span class="n">TensorDataset</span><span class="p">(</span><span class="n">input_ids</span><span class="p">[</span><span class="n">test_idx</span><span class="p">],</span> <span class="n">attention_masks</span><span class="p">[</span><span class="n">test_idx</span><span class="p">],</span> <span class="n">labels</span><span class="p">[</span><span class="n">test_idx</span><span class="p">])</span>

<span class="n">train_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> 
                             <span class="n">sampler</span> <span class="o">=</span> <span class="n">RandomSampler</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">),</span>
                             <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
                             <span class="p">)</span>

<span class="n">validation_dataloader</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">,</span> 
                                  <span class="n">sampler</span> <span class="o">=</span> <span class="n">SequentialSampler</span><span class="p">(</span><span class="n">val_dataset</span><span class="p">),</span>
                                  <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">)</span>

<span class="n">optimizer</span> <span class="o">=</span> <span class="n">AdamW</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span>
                 <span class="n">lr</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span> 
                 <span class="n">eps</span> <span class="o">=</span> <span class="mf">1e-8</span>
                 <span class="p">)</span>

<span class="n">total_steps</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span><span class="o">*</span><span class="n">epochs</span>

<span class="n">scheduler</span> <span class="o">=</span> <span class="n">get_linear_schedule_with_warmup</span><span class="p">(</span><span class="n">optimizer</span><span class="p">,</span> 
                                           <span class="n">num_warmup_steps</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> 
                                           <span class="n">num_training_steps</span> <span class="o">=</span> <span class="n">total_steps</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">flat_accuracy</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="n">pred_flat</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">axis</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">labels_flat</span> <span class="o">=</span> <span class="n">labels</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">pred_flat</span> <span class="o">==</span> <span class="n">labels_flat</span><span class="p">)</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">labels_flat</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">format_time</span><span class="p">(</span><span class="n">elapsed</span><span class="p">):</span>
    <span class="n">elapsed_rounded</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">elapsed</span><span class="p">))</span>
    <span class="k">return</span> <span class="nb">str</span><span class="p">(</span><span class="n">datetime</span><span class="o">.</span><span class="n">timedelta</span><span class="p">(</span><span class="n">seconds</span> <span class="o">=</span> <span class="n">elapsed_rounded</span><span class="p">))</span>

<span class="n">seed_val</span> <span class="o">=</span> <span class="mi">42</span>

<span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>
<span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed_val</span><span class="p">)</span>

<span class="n">training_stats</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">total_t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>

<span class="k">for</span> <span class="n">epoch_i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">epochs</span><span class="p">):</span>

    <span class="n">total_train_loss</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">step</span><span class="p">,</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">):</span>
        
        <span class="k">if</span> <span class="n">step</span> <span class="o">%</span> <span class="mi">40</span> <span class="o">==</span> <span class="mi">0</span> <span class="ow">and</span> <span class="ow">not</span> <span class="n">step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="n">format_time</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>
            
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39; Batch  {:&gt;5, } of {:&gt;5, }.    Elapsed: </span><span class="si">{:}</span><span class="s1">.&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">step</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">),</span> <span class="n">elapsed</span><span class="p">))</span>
                  
        <span class="n">b_input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_input_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="n">model</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
        
        <span class="n">result</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span>
                      <span class="n">token_type_ids</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
                      <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">b_input_mask</span><span class="p">,</span>
                      <span class="n">labels</span> <span class="o">=</span> <span class="n">b_labels</span><span class="p">,</span> 
                      <span class="n">return_dict</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
        
        <span class="n">loss</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">total_train_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>
        
        <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">clip_grad_norm</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="mf">1.0</span><span class="p">)</span>
        
        <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
        <span class="n">scheduler</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
        
    <span class="n">avg_train_loss</span> <span class="o">=</span> <span class="n">total_train_loss</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">train_dataloader</span><span class="p">)</span>
    
    <span class="n">training_time</span> <span class="o">=</span> <span class="n">format_time</span><span class="p">(</span><span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span> <span class="o">-</span> <span class="n">t0</span><span class="p">)</span>

    
    <span class="n">t0</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">eval</span><span class="p">()</span>
    
    <span class="n">total_eval_accuracy</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">total_eval_loss</span> <span class="o">=</span> <span class="mi">0</span>
    <span class="n">nb_eval_steps</span> <span class="o">=</span> <span class="mi">0</span>
    
    <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="n">validation_dataloader</span><span class="p">:</span> 
        <span class="n">b_input_ids</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_input_mask</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">b_labels</span> <span class="o">=</span> <span class="n">batch</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        
        <span class="k">with</span> <span class="n">torch</span><span class="o">.</span><span class="n">no_grad</span><span class="p">():</span>
            
            <span class="n">result</span><span class="o">=</span> <span class="n">mode</span><span class="p">(</span><span class="n">b_input_ids</span><span class="p">,</span>
                        <span class="n">token_type_ids</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> 
                        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">b_input_mask</span><span class="p">,</span> 
                         <span class="n">labels</span> <span class="o">=</span> <span class="n">b_labels</span><span class="p">,</span> 
                         <span class="n">return_dict</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>
            
            
        <span class="n">loss</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">loss</span>
        <span class="n">logits</span> <span class="o">=</span> <span class="n">result</span><span class="o">.</span><span class="n">logits</span>
        
        <span class="n">total_eval_loss</span> <span class="o">+=</span> <span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
        
        <span class="n">logits</span> <span class="o">=</span> <span class="n">logits</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        <span class="n">label_ids</span> <span class="o">=</span> <span class="n">b_labels</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
        
        <span class="n">total_eval_accuarcy</span> <span class="o">+=</span> <span class="n">flat_accuracy</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">label_ids</span><span class="p">)</span>
        
    <span class="n">avg_val_accuracy</span> <span class="o">=</span> <span class="n">total_eval_accuracy</span> <span class="o">/</span> <span class="nb">len</span><span class="p">(</span><span class="n">validation_dataloader</span><span class="p">)</span>

    <span class="n">training_stats</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="p">{</span>
            <span class="s1">&#39;epoch&#39;</span> <span class="p">:</span> <span class="n">epoch_i</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> 
            <span class="s1">&#39;Training Loss&#39;</span> <span class="p">:</span> <span class="n">avg_train_loss</span><span class="p">,</span>
            <span class="s1">&#39;Valid. Loss&#39;</span> <span class="p">:</span> <span class="n">avg_val_loss</span><span class="p">,</span> 
            <span class="s1">&#39;Valid. Accur.&#39;</span> <span class="p">:</span> <span class="n">avg_val_accuracy</span><span class="p">,</span>
            <span class="s1">&#39;Training Time&#39;</span> <span class="p">:</span> <span class="n">training_time</span><span class="p">,</span>
            <span class="s1">&#39;Validation Time&#39;</span><span class="p">:</span> <span class="n">validation_time</span>
        <span class="p">}</span>
    <span class="p">)</span>
                      

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;precision&#39;</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">KFP_stats</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">data</span> <span class="o">=</span> <span class="n">training_stats</span><span class="p">)</span>
<span class="n">KFP_stats</span> <span class="o">=</span> <span class="n">df_stats</span><span class="o">.</span><span class="n">set_index</span><span class="p">(</span><span class="s1">&#39;epoch&#39;</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">style</span><span class="o">.</span><span class="n">set_table_styles</span><span class="p">([</span><span class="nb">dict</span><span class="p">(</span><span class="n">selector</span> <span class="o">=</span> <span class="s1">&#39;th&#39;</span><span class="p">,</span> <span class="n">props</span> <span class="o">=</span> <span class="p">[(</span><span class="s1">&#39;max-width&#39;</span><span class="p">,</span> <span class="s1">&#39;70px&#39;</span><span class="p">)])])</span>
</pre></div>
</div>
<p>Tensorflow + Distilbert</p>
<p>Tips o’ the Hat:</p>
<p><a class="reference external" href="https://towardsdatascience.com/fine-tuning-bert-for-text-classification-54e7df642894">Link1.</a> <a class="reference external" href="https://www.kaggle.com/code/bhavikardeshna/transformer-bert-tensorflow/notebook">Link2.</a> <a class="reference external" href="https://towardsdatascience.com/hugging-face-transformers-fine-tuning-distilbert-for-binary-classification-tasks-490f1d192379">Link3.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-cased&#39;</span><span class="p">)</span>

<span class="n">X_input_ids</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">BText2</span><span class="p">),</span> <span class="mi">256</span><span class="p">))</span>
<span class="n">X_attn_masks</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">BText2</span><span class="p">),</span> <span class="mi">256</span><span class="p">))</span>

<span class="k">def</span> <span class="nf">encode_data</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="n">ids</span><span class="p">,</span> <span class="n">masks</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">df</span><span class="p">[</span><span class="s1">&#39;Text&#39;</span><span class="p">])):</span>
        <span class="n">tokenized_text</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_plus</span><span class="p">(</span>
            <span class="n">text</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> 
            <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
            <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span> 
            <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">return_tensors</span><span class="o">=</span><span class="s1">&#39;tf&#39;</span>
        <span class="p">)</span>
        <span class="n">ids</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">tokenized_text</span><span class="o">.</span><span class="n">input_ids</span>
        <span class="n">masks</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">tokenized_text</span><span class="o">.</span><span class="n">attention_mask</span>
    <span class="k">return</span> <span class="n">ids</span><span class="p">,</span> <span class="n">masks</span>

<span class="n">X_input_ids</span><span class="p">,</span> <span class="n">X_attn_masks</span> <span class="o">=</span> <span class="n">encode_data</span><span class="p">(</span><span class="n">BText2</span><span class="p">,</span> <span class="n">X_input_ids</span><span class="p">,</span> <span class="n">X_attn_masks</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="n">labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">BText2</span><span class="p">),</span> <span class="mi">3</span><span class="p">))</span>
<span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">BText2</span><span class="p">)),</span> <span class="n">BText2</span><span class="p">[</span><span class="s1">&#39;target&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">]</span><span class="o">=</span> <span class="mi">1</span>

<span class="k">def</span> <span class="nf">DatasetMapFunction</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attn_masks</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span>
        \
        <span class="s1">&#39;input_ids&#39;</span><span class="p">:</span> <span class="n">input_ids</span><span class="p">,</span>
        <span class="s1">&#39;attention_mask&#39;</span><span class="p">:</span> <span class="n">attn_masks</span>
    <span class="p">},</span> <span class="n">labels</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">X_input_ids</span><span class="p">,</span> <span class="n">X_attn_masks</span><span class="p">,</span> <span class="n">labels</span><span class="p">))</span>
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">DatasetMapFunction</span><span class="p">)</span>    
<span class="n">dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">10000</span><span class="p">)</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span> <span class="n">drop_remainder</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="n">p</span> <span class="o">=</span> <span class="mf">0.8</span>
<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">((</span><span class="nb">len</span><span class="p">(</span><span class="n">BText2</span><span class="p">)</span><span class="o">//</span><span class="mi">16</span><span class="p">)</span><span class="o">*</span><span class="n">p</span><span class="p">)</span>
<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="n">train_size</span><span class="p">)</span>
<span class="n">val_dataset</span> <span class="o">=</span> <span class="n">dataset</span><span class="o">.</span><span class="n">skip</span><span class="p">(</span><span class="n">train_size</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-cased&#39;</span><span class="p">)</span> 

<span class="n">input_ids</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;input_ids&#39;</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;int32&#39;</span><span class="p">)</span>
<span class="n">attn_masks</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">256</span><span class="p">,</span> <span class="p">),</span> <span class="n">name</span> <span class="o">=</span> <span class="s1">&#39;attention_mask&#39;</span><span class="p">,</span> <span class="n">dtype</span> <span class="o">=</span> <span class="s1">&#39;int32&#39;</span><span class="p">)</span>
<span class="n">bert_embds</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">bert</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attn_masks</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> 
<span class="n">intermediate_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">512</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;relu&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;intermediate_layer&#39;</span><span class="p">)(</span><span class="n">bert_embds</span><span class="p">)</span>
<span class="n">output_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;output_layer&#39;</span><span class="p">)(</span><span class="n">intermediate_layer</span><span class="p">)</span>
<span class="n">feedback_model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span> <span class="o">=</span> <span class="p">[</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">attn_masks</span><span class="p">],</span> <span class="n">outputs</span> <span class="o">=</span> <span class="n">output_layer</span><span class="p">)</span>
<span class="n">feedback_model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">feedback_model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">,</span> <span class="n">decay</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">),</span> 
                        <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> 
                        <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">gpu_options</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">GPUOptions</span><span class="p">(</span><span class="n">per_process_gpu_memory_fraction</span><span class="o">=</span><span class="mf">0.333</span><span class="p">)</span>

<span class="n">sess</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">Session</span><span class="p">(</span><span class="n">config</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">compat</span><span class="o">.</span><span class="n">v1</span><span class="o">.</span><span class="n">ConfigProto</span><span class="p">(</span><span class="n">gpu_options</span><span class="o">=</span><span class="n">gpu_options</span><span class="p">))</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">feedback_model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="p">,</span> 
                            <span class="n">steps_per_epoch</span> <span class="o">=</span> <span class="mi">200</span><span class="p">,</span> 
                            <span class="n">validation_data</span> <span class="o">=</span> <span class="n">val_dataset</span><span class="p">,</span>
                            <span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p>Tensorflow + Autotune + Bert</p>
<p>Tip o’ the Hat:</p>
<p><a class="reference external" href="https://www.kaggle.com/code/imvision12/tensorflow-feedback-bert-baseline">Link1.</a> <a class="reference external" href="https://www.tensorflow.org/tutorials/keras/text_classification">Link2.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">AUTO</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">experimental</span><span class="o">.</span><span class="n">AUTOTUNE</span>
<span class="n">EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">MAX_LEN</span> <span class="o">=</span> <span class="mi">500</span>

<span class="k">def</span> <span class="nf">bert_encode</span><span class="p">(</span><span class="n">texts</span><span class="p">,</span> <span class="n">tokenizer</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="n">MAX_LEN</span><span class="p">):</span>
    <span class="n">input_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">token_type_ids</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="k">for</span> <span class="n">text</span> <span class="ow">in</span> <span class="n">texts</span><span class="p">:</span>
        <span class="n">token</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="n">text</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_len</span><span class="p">,</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">,</span>
                         <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">input_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">[</span><span class="s1">&#39;input_ids&#39;</span><span class="p">])</span>
        <span class="n">token_type_ids</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">[</span><span class="s1">&#39;token_type_ids&#39;</span><span class="p">])</span>
        <span class="n">attention_mask</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">token</span><span class="p">[</span><span class="s1">&#39;attention_mask&#39;</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">input_ids</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">token_type_ids</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">transformers</span><span class="o">.</span><span class="n">BertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;../input/huggingface-bert-variants/bert-base-cased/bert-base-cased&#39;</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">)</span>

<span class="n">sep</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">sep_token</span>
<span class="n">sep</span>

<span class="n">BertPrep</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">BertPrep</span><span class="p">[</span><span class="s1">&#39;Text&#39;</span><span class="p">]</span>

<span class="n">new_label</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;Label&quot;</span><span class="p">:</span> <span class="p">{</span><span class="s2">&quot;Ineffective&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Adequate&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Effective&quot;</span><span class="p">:</span> <span class="mi">2</span><span class="p">}}</span>
<span class="n">BertPrep</span> <span class="o">=</span> <span class="n">BertPrep</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">new_label</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_valid</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_valid</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">BertPrep</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">],</span> <span class="n">BertPrep</span><span class="p">[</span><span class="s1">&#39;Label&#39;</span><span class="p">],</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.12</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">X_train</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">X_train</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">tokenizer</span><span class="p">)</span>
<span class="n">X_valid</span> <span class="o">=</span> <span class="n">bert_encode</span><span class="p">(</span><span class="n">X_valid</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">str</span><span class="p">),</span> <span class="n">tokenizer</span><span class="p">)</span>

<span class="n">y_train</span> <span class="o">=</span> <span class="n">y_train</span><span class="o">.</span><span class="n">values</span>
<span class="n">y_valid</span> <span class="o">=</span> <span class="n">y_valid</span><span class="o">.</span><span class="n">values</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span>
    <span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">))</span>
    <span class="o">.</span><span class="n">repeat</span><span class="p">()</span>
    <span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="mi">2048</span><span class="p">)</span>
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">AUTO</span><span class="p">)</span>
<span class="p">)</span>

<span class="n">valid_dataset</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span>
    <span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="n">X_valid</span><span class="p">,</span> <span class="n">y_valid</span><span class="p">))</span>
    <span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">)</span>
    <span class="o">.</span><span class="n">cache</span><span class="p">()</span>
    <span class="o">.</span><span class="n">prefetch</span><span class="p">(</span><span class="n">AUTO</span><span class="p">)</span>
<span class="p">)</span>

<span class="k">def</span> <span class="nf">build_model</span><span class="p">(</span><span class="n">bert_model</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="n">MAX_LEN</span><span class="p">):</span>    
    <span class="n">input_ids</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;input_ids&quot;</span><span class="p">)</span>
    <span class="n">token_type_ids</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;token_type_ids&quot;</span><span class="p">)</span>
    <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_len</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;attention_mask&quot;</span><span class="p">)</span>

    <span class="n">sequence_output</span> <span class="o">=</span> <span class="n">bert_model</span><span class="p">(</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="o">=</span><span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">clf_output</span> <span class="o">=</span> <span class="n">sequence_output</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">clf_output</span> <span class="o">=</span> <span class="n">Dropout</span><span class="p">(</span><span class="mf">.1</span><span class="p">)(</span><span class="n">clf_output</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">)(</span><span class="n">clf_output</span><span class="p">)</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input_ids</span><span class="p">,</span> <span class="n">token_type_ids</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="n">out</span><span class="p">)</span>
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">Adam</span><span class="p">(</span><span class="n">lr</span><span class="o">=</span><span class="mf">1e-5</span><span class="p">),</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">model</span>

<span class="n">transformer_layer</span> <span class="o">=</span> <span class="p">(</span><span class="n">TFBertModel</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;../input/huggingface-bert-variants/bert-base-cased/bert-base-cased&#39;</span><span class="p">))</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">build_model</span><span class="p">(</span><span class="n">transformer_layer</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="n">MAX_LEN</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">train_history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span>
    <span class="n">train_dataset</span><span class="p">,</span>
    <span class="n">steps_per_epoch</span><span class="o">=</span><span class="mi">200</span><span class="p">,</span>
    <span class="n">validation_data</span><span class="o">=</span><span class="n">valid_dataset</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="mi">5</span>
<span class="p">)</span>
</pre></div>
</div>
<p>TensorFlow + DistilBert2</p>
<p>Tips o’ the Hat:</p>
<p><a class="reference external" href="https://medium.com/geekculture/hugging-face-distilbert-tensorflow-for-custom-text-classification-1ad4a49e26a7">Link1.</a> <a class="reference external" href="https://www.sunnyville.ai/fine-tuning-distilbert-multi-class-text-classification-using-transformers-and-tensorflow/">Link2.</a> <a class="reference external" href="https://www.tensorflow.org/text/tutorials/classify_text_with_bert">Link3.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Label_cats&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Labels&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="s1">&#39;category&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">cat</span><span class="o">.</span><span class="n">codes</span>

<span class="n">pd</span><span class="o">.</span><span class="n">set_option</span><span class="p">(</span><span class="s1">&#39;display.max_colwidth&#39;</span><span class="p">,</span> <span class="kc">None</span><span class="p">)</span>
<span class="n">MODEL_NAME</span> <span class="o">=</span> <span class="s1">&#39;distilbert-base-uncased-finetuned-sst-2-english&#39;</span>
<span class="n">BATCH_SIZE</span> <span class="o">=</span> <span class="mi">16</span>
<span class="n">N_EPOCHS</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">data_texts</span> <span class="o">=</span> <span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Token&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="n">data_labels</span> <span class="o">=</span> <span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Label_cats&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

<span class="n">train_texts</span><span class="p">,</span> <span class="n">val_texts</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">data_texts</span><span class="p">,</span> <span class="n">data_labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Keep some data for inference (testing)</span>
<span class="n">train_texts</span><span class="p">,</span> <span class="n">test_texts</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_labels</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">train_texts</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Token&#39;</span><span class="p">],</span> <span class="n">feature_sentences</span><span class="p">[</span><span class="s1">&#39;Labels&#39;</span><span class="p">],</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.3</span><span class="p">)</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">DistilBertTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">MODEL_NAME</span><span class="p">)</span>

<span class="n">train_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">train_texts</span><span class="p">),</span>
                           <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                           <span class="n">padding</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">val_encodings</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">val_texts</span><span class="p">),</span>
                          <span class="n">truncation</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
                          <span class="n">padding</span> <span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">train_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="nb">dict</span><span class="p">(</span><span class="n">train_encodings</span><span class="p">),</span> <span class="n">train_labels</span><span class="p">))</span>

<span class="n">test_dataset</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="o">.</span><span class="n">from_tensor_slices</span><span class="p">((</span><span class="nb">dict</span><span class="p">(</span><span class="n">val_encodings</span><span class="p">),</span> <span class="n">val_labels</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">TFDistilBertForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="s1">&#39;distilbert-base-uncased&#39;</span><span class="p">,</span> <span class="n">num_labels</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span>

<span class="n">O_ptimizer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">optimizers</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">5e-5</span><span class="p">)</span>

<span class="n">l0ss</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">losses</span><span class="o">.</span><span class="n">SparseCategoricalCrossentropy</span><span class="p">(</span><span class="n">from_logits</span><span class="o">=</span> <span class="kc">True</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">O_ptimizer</span><span class="p">,</span> <span class="n">loss</span> <span class="o">=</span> <span class="n">l0ss</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_dataset</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">X_train</span><span class="p">))</span><span class="o">.</span><span class="n">batch</span><span class="p">(</span><span class="n">BATCH_SIZE</span><span class="p">),</span> <span class="n">epochs</span> <span class="o">=</span> <span class="n">N_EPOCHS</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">BATCH_SIZE</span><span class="p">)</span>
</pre></div>
</div>
<p>Keras + LSTM 1</p>
<p>Tip o’ the Hat:</p>
<p><a class="reference external" href="https://www.tensorflow.org/tutorials/keras/text_classification">Link.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">10000</span>
<span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">max_length</span> <span class="o">=</span> <span class="mi">250</span>
<span class="n">trunc_type</span> <span class="o">=</span> <span class="s1">&#39;post&#39;</span>
<span class="n">padding_type</span> <span class="o">=</span> <span class="s1">&#39;post&#39;</span>
<span class="n">oov_tok</span> <span class="o">=</span> <span class="s1">&#39;&lt;OOV&gt;&#39;</span>
<span class="n">training_portion</span> <span class="o">=</span> <span class="mf">.8</span>

<span class="n">items</span> <span class="o">=</span> <span class="n">PreppedText</span>
<span class="n">labels</span> <span class="o">=</span> <span class="n">PreppedLabels</span>

<span class="n">train_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">PreppedText</span><span class="p">)</span><span class="o">*</span> <span class="n">training_portion</span><span class="p">)</span>
<span class="n">train_items</span> <span class="o">=</span> <span class="n">items</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span> <span class="n">train_size</span><span class="p">]</span>
<span class="n">train_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span> <span class="n">train_size</span><span class="p">]</span>

<span class="n">validation_items</span> <span class="o">=</span> <span class="n">items</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>
<span class="n">validation_labels</span> <span class="o">=</span> <span class="n">labels</span><span class="p">[</span><span class="n">train_size</span><span class="p">:]</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span> <span class="o">=</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">oov_token</span> <span class="o">=</span> <span class="n">oov_tok</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">train_items</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>

<span class="nb">dict</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">())[</span><span class="mi">10</span><span class="p">:</span><span class="mi">20</span><span class="p">])</span>

<span class="n">train_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">train_items</span><span class="p">)</span>
<span class="n">train_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">train_sequences</span><span class="p">,</span> <span class="n">maxlen</span><span class="o">=</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="n">padding_type</span><span class="p">,</span> <span class="n">truncating</span> <span class="o">=</span> <span class="n">trunc_type</span><span class="p">)</span>
<span class="n">validation_sequences</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">validation_items</span><span class="p">)</span>
<span class="n">validation_padded</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">validation_sequences</span><span class="p">,</span> <span class="n">maxlen</span> <span class="o">=</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">padding</span> <span class="o">=</span> <span class="n">padding_type</span><span class="p">,</span> <span class="n">truncating</span> <span class="o">=</span> <span class="n">trunc_type</span> <span class="p">)</span>

<span class="n">label_tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">()</span>
<span class="n">label_tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>

<span class="n">training_label_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">label_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">train_labels</span><span class="p">))</span>
<span class="n">validation_label_seq</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">label_tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">validation_labels</span><span class="p">))</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">([</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">)),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">embedding_dim</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;relu&#39;</span><span class="p">),</span>
    <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">6</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;softmax&#39;</span><span class="p">)</span>
<span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;sparse_categorical_crossentropy&#39;</span><span class="p">,</span>
             <span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span>
             <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="n">num_epochs</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">train_padded</span><span class="p">,</span> 
                    <span class="n">training_label_seq</span><span class="p">,</span> 
                    <span class="n">epochs</span> <span class="o">=</span> <span class="n">num_epochs</span><span class="p">,</span>
                    <span class="n">validation_data</span> <span class="o">=</span> <span class="p">(</span><span class="n">validation_padded</span><span class="p">,</span>
                                      <span class="n">validation_label_seq</span><span class="p">),</span>
                    <span class="n">verbose</span> <span class="o">=</span> <span class="mi">2</span><span class="p">)</span>
</pre></div>
</div>
<p>LSTM v. 2 Tips o’ the Hat:</p>
<ul class="simple">
<li><p><a class="reference external" href="https://adventuresinmachinelearning.com/keras-lstm-tutorial/">Link1</a>.</p></li>
<li><p><a class="reference external" href="https://towardsdatascience.com/multi-class-text-classification-with-lstm-1590bee1bd17">Link2</a>.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">MAX_NB_WORDS</span> <span class="o">=</span> <span class="mi">50000</span>
<span class="n">MAX_SEQUENCE_LENGTH</span> <span class="o">=</span> <span class="mi">400</span>
<span class="n">EMBEDDING_DIM</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">tokenizer</span> <span class="o">=</span> <span class="n">Tokenizer</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="n">MAX_NB_WORDS</span><span class="p">,</span> <span class="n">filters</span><span class="o">=</span><span class="s1">&#39;!&quot;#$%&amp;()*+,-./:;&lt;=&gt;?@[\]^_`{|}~&#39;</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">tokenizer</span><span class="o">.</span><span class="n">fit_on_texts</span><span class="p">(</span><span class="n">KFP_df</span><span class="p">[</span><span class="s1">&#39;Token&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">word_index</span>

<span class="n">X</span> <span class="o">=</span> <span class="n">tokenizer</span><span class="o">.</span><span class="n">texts_to_sequences</span><span class="p">(</span><span class="n">KFP_df</span><span class="p">[</span><span class="s1">&#39;Token&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">values</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">pad_sequences</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">maxlen</span> <span class="o">=</span> <span class="n">MAX_SEQUENCE_LENGTH</span><span class="p">)</span>
<span class="n">Y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">KFP_df</span><span class="p">[</span><span class="s1">&#39;Labels&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">values</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">Y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span><span class="n">Y</span><span class="p">,</span> <span class="n">test_size</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>


<span class="n">model</span> <span class="o">=</span> <span class="n">Sequential</span><span class="p">()</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Embedding</span><span class="p">(</span><span class="n">MAX_NB_WORDS</span><span class="p">,</span> <span class="n">EMBEDDING_DIM</span><span class="p">,</span> <span class="n">input_length</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">SpatialDropout1D</span><span class="p">(</span><span class="mf">0.2</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="n">dropout</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span> <span class="n">recurrent_dropout</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">Dense</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">activation</span> <span class="o">=</span> <span class="s1">&#39;softmax&#39;</span><span class="p">))</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">loss</span> <span class="o">=</span> <span class="s1">&#39;categorical_crossentropy&#39;</span><span class="p">,</span> <span class="n">optimizer</span> <span class="o">=</span> <span class="s1">&#39;adam&#39;</span><span class="p">,</span> <span class="n">metrics</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;accuracy&#39;</span><span class="p">])</span>

<span class="nb">print</span><span class="p">(</span><span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">())</span>

<span class="n">epochs</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>

<span class="n">history</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">Y_train</span><span class="p">,</span> <span class="n">epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">,</span> <span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> <span class="n">validation_split</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span><span class="n">EarlyStopping</span><span class="p">(</span><span class="n">monitor</span> <span class="o">=</span> <span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">patience</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">min_delta</span> <span class="o">=</span> <span class="mf">0.0001</span><span class="p">)])</span>

<span class="n">accr</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">Y_test</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Test set</span><span class="se">\n</span><span class="s1"> Loss: </span><span class="si">{:0.3f}</span><span class="se">\n</span><span class="s1"> Accuracy: </span><span class="si">{:0.3f}</span><span class="s1">&#39;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">accr</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">accr</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">Model</span><span class="p">:</span> <span class="s2">&quot;sequential&quot;</span>
<span class="n">_________________________________________________________________</span>
 <span class="n">Layer</span> <span class="p">(</span><span class="nb">type</span><span class="p">)</span>                <span class="n">Output</span> <span class="n">Shape</span>              <span class="n">Param</span> <span class="c1">#   </span>
<span class="o">=================================================================</span>
 <span class="n">embedding</span> <span class="p">(</span><span class="n">Embedding</span><span class="p">)</span>       <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>          <span class="mi">5000000</span>   
                                                                 
 <span class="n">spatial_dropout1d</span> <span class="p">(</span><span class="n">SpatialD</span>  <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">400</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>         <span class="mi">0</span>         
 <span class="n">ropout1D</span><span class="p">)</span>                                                       
                                                                 
 <span class="n">lstm</span> <span class="p">(</span><span class="n">LSTM</span><span class="p">)</span>                 <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>               <span class="mi">80400</span>     
                                                                 
 <span class="n">dense</span> <span class="p">(</span><span class="n">Dense</span><span class="p">)</span>               <span class="p">(</span><span class="kc">None</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>                 <span class="mi">303</span>       
                                                                 
<span class="o">=================================================================</span>
<span class="n">Total</span> <span class="n">params</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span><span class="mi">080</span><span class="p">,</span><span class="mi">703</span>
<span class="n">Trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">5</span><span class="p">,</span><span class="mi">080</span><span class="p">,</span><span class="mi">703</span>
<span class="n">Non</span><span class="o">-</span><span class="n">trainable</span> <span class="n">params</span><span class="p">:</span> <span class="mi">0</span>
<span class="n">_________________________________________________________________</span>
<span class="kc">None</span>

<span class="n">Epoch</span> <span class="mi">1</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">414</span><span class="o">/</span><span class="mi">414</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1188</span><span class="n">s</span> <span class="mi">3</span><span class="n">s</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.8526</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.6225</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.7720</span> <span class="o">-</span> <span class="n">val_accuracy</span><span class="p">:</span> <span class="mf">0.6560</span>
<span class="n">Epoch</span> <span class="mi">2</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">414</span><span class="o">/</span><span class="mi">414</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1144</span><span class="n">s</span> <span class="mi">3</span><span class="n">s</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.7044</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.6924</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.7799</span> <span class="o">-</span> <span class="n">val_accuracy</span><span class="p">:</span> <span class="mf">0.6482</span>
<span class="n">Epoch</span> <span class="mi">3</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">414</span><span class="o">/</span><span class="mi">414</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1296</span><span class="n">s</span> <span class="mi">3</span><span class="n">s</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.6040</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.7445</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.8278</span> <span class="o">-</span> <span class="n">val_accuracy</span><span class="p">:</span> <span class="mf">0.6363</span>
<span class="n">Epoch</span> <span class="mi">4</span><span class="o">/</span><span class="mi">5</span>
<span class="mi">414</span><span class="o">/</span><span class="mi">414</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1422</span><span class="n">s</span> <span class="mi">3</span><span class="n">s</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.5146</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.7878</span> <span class="o">-</span> <span class="n">val_loss</span><span class="p">:</span> <span class="mf">0.9018</span> <span class="o">-</span> <span class="n">val_accuracy</span><span class="p">:</span> <span class="mf">0.6237</span>

<span class="mi">230</span><span class="o">/</span><span class="mi">230</span> <span class="p">[</span><span class="o">==============================</span><span class="p">]</span> <span class="o">-</span> <span class="mi">79</span><span class="n">s</span> <span class="mi">344</span><span class="n">ms</span><span class="o">/</span><span class="n">step</span> <span class="o">-</span> <span class="n">loss</span><span class="p">:</span> <span class="mf">0.8855</span> <span class="o">-</span> <span class="n">accuracy</span><span class="p">:</span> <span class="mf">0.6290</span>
<span class="n">Test</span> <span class="nb">set</span>
 <span class="n">Loss</span><span class="p">:</span> <span class="mf">0.886</span>
 <span class="n">Accuracy</span><span class="p">:</span> <span class="mf">0.629</span>
</pre></div>
</div>
<p>Five-Fold DeBERTa + AutoModel</p>
<p>Tips o’ the Hat:</p>
<p><a class="reference external" href="https://www.kaggle.com/code/tdh512194/simple-5-foldcv-deberta-baseline/notebook">Link1.</a></p>
<p><a class="reference external" href="https://towardsdatascience.com/three-out-of-the-box-transformer-models-4bc4880bc992">Link2.</a></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">set_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">):</span>
    <span class="n">os</span><span class="o">.</span><span class="n">environ</span><span class="p">[</span><span class="s2">&quot;PL_GLOBAL_SEED&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">str</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">cuda</span><span class="o">.</span><span class="n">manual_seed_all</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>
    
<span class="n">seed_everything</span><span class="p">(</span><span class="mi">512</span><span class="p">)</span>


<span class="n">model_name</span> <span class="o">=</span> <span class="s1">&#39;../input/debertav3base&#39;</span>

<span class="n">tokenz</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">)</span>
<span class="n">sep</span> <span class="o">=</span> <span class="n">tokenz</span><span class="o">.</span><span class="n">sep_token</span>

<span class="n">df</span> <span class="o">=</span> <span class="n">train</span>
<span class="n">df</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">]</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">Text</span>
<span class="n">new_label</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Label&quot;</span><span class="p">:</span> <span class="p">{</span>
        <span class="s2">&quot;Ineffective&quot;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Adequate&quot;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;Effective&quot;</span><span class="p">:</span> <span class="mi">2</span>
    <span class="p">}</span>
<span class="p">}</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="n">new_label</span><span class="p">)</span>
<span class="n">df</span> <span class="o">=</span> <span class="n">df</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Label&#39;</span><span class="p">:</span> <span class="s1">&#39;label&#39;</span><span class="p">})</span>
<span class="n">ds</span> <span class="o">=</span> <span class="n">Dataset</span><span class="o">.</span><span class="n">from_pandas</span><span class="p">(</span><span class="n">df</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">tokenize_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">tokenz</span><span class="p">(</span><span class="n">x</span><span class="p">[</span><span class="s1">&#39;inputs&#39;</span><span class="p">],</span> <span class="n">truncation</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">max_length</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> 
                <span class="n">padding</span><span class="o">=</span><span class="s1">&#39;max_length&#39;</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">score</span><span class="p">(</span><span class="n">preds</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="s1">&#39;log loss&#39;</span><span class="p">:</span> <span class="n">log_loss</span><span class="p">(</span>
        <span class="n">preds</span><span class="o">.</span><span class="n">label_ids</span><span class="p">,</span> 
        <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="n">preds</span><span class="o">.</span><span class="n">predictions</span><span class="p">))</span>
    <span class="p">)}</span>

<span class="n">tokens_ds</span> <span class="o">=</span> <span class="n">ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">tokenize_function</span><span class="p">,</span> 
                <span class="n">batched</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> 
                <span class="n">remove_columns</span><span class="o">=</span><span class="p">(</span><span class="s1">&#39;discourse_text&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;discourse_type&#39;</span><span class="p">,</span> 
                                <span class="s1">&#39;inputs&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;discourse_id&#39;</span><span class="p">,</span>
                                <span class="s1">&#39;essay_id&#39;</span><span class="p">))</span>


<span class="n">kfold</span> <span class="o">=</span> <span class="n">StratifiedGroupKFold</span><span class="p">(</span><span class="n">n_splits</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>

<span class="n">dds_folds</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">train_idxs</span><span class="p">,</span> <span class="n">val_idxs</span> <span class="ow">in</span> <span class="n">kfold</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">index</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="p">,</span> <span class="n">df</span><span class="o">.</span><span class="n">essay_id</span><span class="p">):</span>
    <span class="n">dds_folds</span><span class="o">.</span><span class="n">append</span><span class="p">(</span>
        <span class="n">DatasetDict</span><span class="p">({</span>
        <span class="s1">&#39;train&#39;</span><span class="p">:</span><span class="n">tokens_ds</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">trn_idxs</span><span class="p">),</span> 
        <span class="s1">&#39;test&#39;</span><span class="p">:</span> <span class="n">tokens_ds</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">val_idxs</span><span class="p">)</span>
    <span class="p">}))</span>

<span class="n">learning_rate</span> <span class="o">=</span> <span class="mf">3e-5</span>
<span class="n">batch_size</span> <span class="o">=</span>  <span class="mi">16</span>
<span class="n">weight_decay</span> <span class="o">=</span> <span class="mf">0.01</span>
<span class="n">epochs</span> <span class="o">=</span>  <span class="mi">2</span>

<span class="k">for</span> <span class="n">fold</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">5</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Training fold </span><span class="si">{</span><span class="n">fold</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">TrainingArguments</span><span class="p">(</span>
        <span class="n">output_dir</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;outputs/fold_</span><span class="si">{</span><span class="n">fold</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">,</span>
        <span class="n">learning_rate</span> <span class="o">=</span> <span class="n">learning_rate</span><span class="p">,</span> 
        <span class="n">warmup_ratio</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> 
        <span class="n">lr_scheduler_type</span> <span class="o">=</span><span class="s1">&#39;cosine&#39;</span><span class="p">,</span> 
        <span class="n">num_train_epochs</span> <span class="o">=</span> <span class="n">epochs</span><span class="p">,</span> 
        <span class="n">weight_decay</span> <span class="o">=</span> <span class="n">weight_decay</span><span class="p">,</span> 
        <span class="n">report_to</span> <span class="o">=</span> <span class="s1">&#39;none&#39;</span><span class="p">,</span>
        <span class="n">evaluation_strategy</span> <span class="o">=</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span> 
        <span class="n">save_strategy</span> <span class="o">=</span> <span class="s2">&quot;no&quot;</span><span class="p">,</span>
        <span class="n">label_smoothing_factor</span> <span class="o">=</span> <span class="mf">0.05</span><span class="p">,</span>
        <span class="n">per_device_train_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span> 
        <span class="n">per_device_eval_batch_size</span> <span class="o">=</span> <span class="n">batch_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">model</span> <span class="o">=</span> <span class="n">AutoModelForSequenceClassification</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> 
                                                               <span class="n">num_labels</span><span class="o">=</span><span class="n">df</span><span class="o">.</span><span class="n">label</span><span class="o">.</span><span class="n">nunique</span><span class="p">())</span>
    <span class="n">trainer</span> <span class="o">=</span> <span class="n">Trainer</span><span class="p">(</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span><span class="p">,</span> 
                      <span class="n">args</span> <span class="o">=</span> <span class="n">args</span><span class="p">,</span> 
                   <span class="n">train_dataset</span> <span class="o">=</span> <span class="n">dds_folds</span><span class="p">[</span><span class="n">fold</span><span class="p">][</span><span class="s1">&#39;train&#39;</span><span class="p">],</span> 
                      <span class="n">eval_dataset</span> <span class="o">=</span> <span class="n">dds_folds</span><span class="p">[</span><span class="n">fold</span><span class="p">][</span><span class="s1">&#39;test&#39;</span><span class="p">],</span> 
                  <span class="n">compute_metrics</span><span class="o">=</span><span class="n">score</span><span class="p">)</span>

    <span class="n">trainer</span><span class="o">.</span><span class="n">train</span><span class="p">()</span>
    <span class="n">model</span><span class="o">.</span><span class="n">save_pretrained</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;outputs/fold_</span><span class="si">{</span><span class="n">fold</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>This model was the highest-performing of all the models (and quite a few more) that I’ve included in this overview.</p>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="Ensembling.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Ensembles</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="Conclusion.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Conclusion</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Marcus Goff<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>